# This reloads the extension if already loaded - Everytime you update the .py files, re-run this cell!
%reload_ext autoreload
# Automatically reloads modules before executing code OR makes Jupyter reload your .py files whenever you run a cell.
%autoreload 2


# Importing all of the required python packages
import sys
import os


# Setting up the folder paths.
sys.path.append('./lib')

# Import resuable code (scripts) within .py files.
from data_loader import load_csv, save_df_to_csv
from data_wrangle import add_cols, df_formater, remove_cols, remove_rows, col_name_changer, df_split





df_indicators = load_csv('./data/raw/U.S._Chronic_Disease_Indicators.csv')
# print(df.head())


# df2 = load_csv('./data/raw/US_Census_Data_2022_v01.csv')
# print(df2.head())


df_census = load_csv('./data/raw/US_Census_Data_2022_v03_transpose.csv')
# display(df3.head())








col_names = ['State']
df_census_temp0 = add_cols(df_census, col_names)
# display(df_census_temp0)    # For debugging only - comment out when not needed.


df_census_temp1 = df_formater(df_census_temp0)
# display(df_census_temp1)    # For debugging only - comment out when not needed.


#df_census_temp2 = remove_cols(df_census_temp1)
display(df_census_temp2)    # For debugging only - comment out when not needed.


#df_census_temp3 = remove_rows(df_census_temp2)
display(df_census_temp3)    # For debugging only - comment out when not needed.


og_string = "!!"
new_string = " -- "
df_census_temp4 = col_name_changer(df_census_temp3, og_string, new_string)
# display(df_census_temp4)     # For debugging only - comment out when not needed.


df_state_only, df_state_city = df_split(df_census_temp4)
# display(df_state_only)    # For debugging only - comment out when not needed.
# display(df_state_city)    # For debugging only - comment out when not needed.


save_df_to_csv(df_state_only, './data/processed/US_Census_Data_2022_state_only.csv')


save_df_to_csv(df_state_city, './data/processed/US_Census_Data_2022_city_state.csv')





# Importing visual.py for creating the EDA visualizations

from lib import visual as vis
import pandas as pd
#import missingno as msno
import matplotlib.pyplot as plt
import seaborn as sns


# Loading final dataframe

df1 = pd.read_csv ('./data/processed/Final_dataset.csv')
df = df1.copy() # Creating a copy to avoid changing the underlying dataset


# Confirming the number of rows and columns

df.shape


# Obtaining detailed information about the DataFrame

df.info()


# Generating descriptive statistics for numeric variables

df.describe().T


# Checking a fequency-based summary of the object data types
df.describe(include="object").T


# select object columns except State
obj_cols = df.select_dtypes(include="object").columns.drop("State")

# convert to numeric (remove commas, coerce errors to NaN)
df[obj_cols] = df[obj_cols].apply(
    lambda col: pd.to_numeric(col.str.replace(",", ""), errors="coerce")
)


df.shape


# Checking for duplicate values

df.duplicated().sum()


# Identifying the percent of missing values in the dataset

df.isnull().sum().sum() / df.size





# 1. Calculate the percentage of missing values
missing_pct = df.isnull().mean() * 100

# 2. Define the buckets based on your criteria
# We create a dictionary where the key is the Sheet Name and the value is the filtered data
report_data = {
    "Safe_0-5_pct": missing_pct[(missing_pct >= 0) & (missing_pct <= 5)],
    "Attention_5-20_pct": missing_pct[(missing_pct > 5) & (missing_pct <= 20)],
    "Risky_20-40_pct": missing_pct[(missing_pct > 20) & (missing_pct <= 40)],
    "High_Missing_Over_40": missing_pct[missing_pct > 40],
    "Near_Empty_Over_90": missing_pct[missing_pct > 90]
}

# 3. Export to Excel
file_name = "Missing_Value_Report.xlsx"

with pd.ExcelWriter(file_name) as writer:
    for sheet_name, data in report_data.items():
        # Convert the Series to a DataFrame and sort it
        df_category = data.reset_index()
        df_category.columns = ['Column_Name', 'Missing_Percentage']
        df_category = df_category.sort_values(by='Missing_Percentage', ascending=False)
        
        # Save to the specific sheet
        df_category.to_excel(writer, sheet_name=sheet_name, index=False)

print(f"Done! Your report '{file_name}' has been created in your current folder.")


# Dropping columns that have more than 5% of missing values

threshold = 0.05  # 5%

cols_to_drop = df.columns[df.isna().mean() > threshold]

df = df.drop(columns=cols_to_drop)

df.shape


# Dropping ...

def drop_columns(df, column_strings):
    """
        Searches for certain strings in column names and drops those columns
    
        Parameters
        ----------
        df : pandas.DataFrame
        columns_strings : list of strings to search for in the column names 
 
        Returns
        -------
        pandas.DataFrame
            A dataframe without the columns that had the strings specified
            
    """

    df_dropped = df.copy()
    for search_string in column_strings: 
        df_dropped = df_dropped.drop([col for col in df_dropped.columns if search_string in col], axis = 1)

    return df_dropped

cols_to_drop = ["White", "Black", "Hispanic", "Hawaiian or Pacific Islander",
                       "American Indian or Alaska Native", "Multiracial", "Asian", "ConfidenceLimit",
               "moe - "]
df = drop_columns(df, cols_to_drop)

# cols_to_drop = df.drop([col for col in df.columns if 'moe - ' in col], axis = 1)
# cols_to_drop2 = cols_to_drop.drop([col for col in cols_to_drop.columns if 'ConfidenceLimit' in col], axis = 1)
# race_list = ["White", "Black", "Hispanic", "Hawaiian or Pacific Islander",
#             "American Indian or Alaska Native", "Multiracial", "Asian"]

# cols_to_drop3 = cols_to_drop2.drop(columns= race_list, axis = 1)





df.columns


vis_cols = list(df.columns[1:])
for vis in vis_cols:
    histogram_boxplot(df, vis)








#histogram_boxplot(df, 'est - Total Pop')







